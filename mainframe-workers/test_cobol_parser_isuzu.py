"""
Description: 
A test script designed to parse COBOL Isuzu files with support for parallelism.

Usages:
1. Run the test script:
   `python test_cobol_parser_isuzu.py <folder_path>`

2. Check for COBOL program errors when failing to export JSON files:
   `cat <folder_path>/failed_files.log`

3. Remove all JSON files generated by the test script:
   `rm -rf <folder_path>/*.json && rm -rf <folder_path>/failed_files.log`

"""

import os
import re
import sys
import json
import codecs
import chardet

import warnings

# Ignore all warnings
warnings.filterwarnings("ignore")

from tqdm.contrib.concurrent import process_map
from antlr4 import CommonTokenStream, InputStream
from antlr4.error.ErrorListener import ErrorListener
from grammar.isuzu_cobol.CobolIsuzuLexer import CobolIsuzuLexer
from grammar.isuzu_cobol.CobolIsuzuParser import CobolIsuzuParser
from grammar.isuzu_cobol.CobolIsuzuVisitorImp import CobolIsuzuVisitorImp
from grammar.isuzu_cobol.isuzu_cobol_schemas import DNPStatementFactory

# Initalize
max_workers = 20 # multiprocessing.cpu_count()

# Custom listener for checking the errors of Lexer (token errors)
class QuietErrorListener(ErrorListener):

    def __init__(self, verbose = False):
        super(QuietErrorListener, self).__init__()

    def syntaxError(self, recognizer, offendingSymbol, line, column, msg, e):
        pass

    def reportAmbiguity(self, recognizer, dfa, startIndex, stopIndex, exact, ambigAlts, configs):
        pass

    def reportAttemptingFullContext(self, recognizer, dfa, startIndex, stopIndex, conflictingAlts, configs):
        pass

    def reportContextSensitivity(self, recognizer, dfa, startIndex, stopIndex, prediction, configs):
        pass


# This function inherits from the repository parser
def beautify_lisp_string(in_string):
    indent_size = 3
    add_indent = ' '*indent_size
    out_string = in_string[0]  # no indent for 1st (
    indent = ''
    for i in range(1, len(in_string)):
        if in_string[i] == '(' and in_string[i+1] != ' ':
            indent += add_indent
            out_string += "\n" + indent + '('
        elif in_string[i] == ')':
            out_string += ')'
            if len(indent) > 0:
                indent = indent.replace(add_indent, '', 1)
        else:
            out_string += in_string[i]
    return out_string


# This function inherits from the repository parser
def preprocess_cbl_isuzu(code: str) -> str:
    L = 72  # Length of code line

    # Replace the token
    code = code.replace("\u001E", "<RS>")

    # Split lines and define pattern for identification
    lines = code.splitlines()
    pattern = re.compile(r"^.{7}IDENTIFICATION\s*DIVISION.|^.{7}ID\s*DIVISION.|^.{8}IDENTIFICATION\s*DIVISION.|^.{8}ID\s*DIVISION.")

    # Determine the starting point of relevant code
    have_code_line = False
    for i, line in enumerate(lines):
        if line.startswith("00"):
            have_code_line = True
        if pattern.search(line):
            break
        if i > 100:
            break

    if i <= 100:
        code = "\n".join(lines[i:])

    # Add line numbers if missing
    if not have_code_line:
        lines = code.splitlines()
        lines = ["000000" + line.strip() for line in lines]
        code = "\n".join(lines)

    # Standardize line prefixes
    code = re.sub(r"^.{6}", "      ", code, flags=re.MULTILINE)
    code = re.sub(r"^.{6}\/", "      *", code, flags=re.MULTILINE)
    code = re.sub(r"^\s{7}\*", "      **", code, flags=re.MULTILINE)
    code = re.sub(r'\*>.*$', '', code, flags=re.MULTILINE)

    # Remove empty lines and trim to the required length
    code = "\n".join(line[:L] for line in code.splitlines() if line.strip())

    # Remove trailing whitespace
    code = "\n".join(line.rstrip() for line in code.splitlines() if line.strip())

    # Define patterns to truncate and their respective lengths
    patterns_to_truncate = {
        "      000": 4,
        "        0212": 4,
        "         9403": 4,
        "          9707": 4,
        "          9702": 4,
        "        970": 4,
        "        0502": 4,
        "    A": 4,
        "    D": 4,
        "      ****": 4,
        "T               00": 4,
        "T                   00": 4,
        "        100107": 6
    }

    # Apply truncation based on patterns
    lines = code.splitlines()
    for i, line in enumerate(lines):
        for pattern, truncate_len in patterns_to_truncate.items():
            if line.endswith(pattern):
                lines[i] = line[:-truncate_len]
                break

    # Compile pattern to remove trailing patterns
    trailing_pattern = re.compile(r'\.\s{2,}[\d/]+$')

    # Remove trailing patterns
    cleaned_lines = [re.sub(trailing_pattern, '.', line) for line in lines]

    return "\n".join(cleaned_lines)



def read_file(file_path: str) -> str:
    """
    Reads the content of a file, detects its encoding, and returns the decoded string.
    
    Parameters:
        file_path (str): The path to the file to be read.
        
    Returns:
        str: The content of the file as a decoded string.
    """
    encoding = "utf-8"  # Default encoding

    # Detect file encoding
    try:
        with open(file_path, 'rb') as file:
            result = chardet.detect(file.read())
            encoding = result.get('encoding', 'utf-8')  # Use detected encoding or default to UTF-8
    except Exception as e:
        print(f"Error detecting file encoding: {e}")

    # Read and decode file content
    try:
        with codecs.open(file_path, "rb") as f:
            raw_data = f.read()
            try:
                return raw_data.decode(encoding, errors="replace").replace("\uFFFD", "")
            except UnicodeDecodeError as e:
                print(f"Error decoding with detected encoding ({encoding}): {e}")
                # Fallback to UTF-8 decoding
                return raw_data.decode("utf-8", errors="replace").replace("\uFFFD", "")
    except Exception as e:
        print(f"Error reading file: {e}")
        raise IOError(f"Failed to read or decode the file at {file_path}.") from e


def handle_variable_list(variable_list):
    output = {}
    for variable in variable_list:
        n, L  = variable # field and list of objects
        output[n] = [l.dict() for l in L]
    return output


def parse_cbl(file_path:str)->None:

    # Get file name
    file_name = file_path.split("/")[-1]
    file_name_no_tail = file_name.split(".")[0]
    # Get contain folder
    folder_path = os.path.dirname(file_path)
    
    # Read file
    code = read_file(file_path)

    # Preprocess
    code = preprocess_cbl_isuzu(code)

    # Run lexer
    stream = InputStream(code)
    lexer = CobolIsuzuLexer(stream)
    lexer.removeErrorListeners()  # Remove default error listeners
    lexer.addErrorListener(QuietErrorListener())  # Add custom error listener

    stream = CommonTokenStream(lexer)
    stream.fill()

    # Run parser
    parser = CobolIsuzuParser(stream)
    parser.removeErrorListeners()  # Remove default error listeners
    parser.addErrorListener(QuietErrorListener())  # Add custom error listener
    parser.buildParseTrees = True

    # Build tree
    tree = parser.startRule()

    # Visit tree and Collect Information
    visitor = CobolIsuzuVisitorImp(parser=parser)
    visitor.visit(tree)

    statements = [DNPStatementFactory.from_dict(statement) for statement in visitor.statements]

    parsed_program = {
        "copybook_list": [copybook.dict() for copybook in visitor.copybook_list],
        "called_program_list": [program.dict() for program in visitor.called_program_list],
        "file_control_list": [control.dict() for control in visitor.file_control_list],
        "file_description_list": [description.dict() for description in visitor.file_description_list],
        "variable_list": handle_variable_list(visitor.variable_list),
        "division_list": [division.dict() for division in visitor.division_list],
        "section_list": [section.dict() for section in visitor.section_list],
        "paragraph_list": [paragraph.dict() for paragraph in visitor.paragraph_list],
        "statements": [statement.dict() for statement in statements]

    }

    # Save json file
    output_json_path = os.path.join(folder_path,f"{file_name_no_tail}.json")
    with open(output_json_path,"w", encoding="utf-8") as f:
        json.dump(parsed_program, f, ensure_ascii=False, indent=4)

    #print(f"Parsed program saved to {output_json_path}")


def log_failed_file(folder_path: str, file_path: str, error: str):
    """
    Logs details of files that failed during processing.

    Args:
        folder_path (str): The folder where the log file is saved.
        file_path (str): The file path of the failed file.
        error (str): The error message.
    """
    log_file_path = os.path.join(folder_path, "failed_files.log")
    with open(log_file_path, "a", encoding="utf-8") as log_file:
        log_file.write(f"Failed to process {file_path}: {error}\n")


def process_file(file_path):
    folder_path = os.path.dirname(file_path)
    try:
        parse_cbl(file_path)
    except Exception as e:
        #print(f"Error parsing {file_path}: {e}")
        log_failed_file(folder_path, file_path, str(e))


if __name__ == "__main__":

    folder_path = "tmp"

    if folder_path:
        print("Running...")
        # Get all files
        files = os.listdir(folder_path)
        # Filter files
        cobol_files = [f for f in files if f.endswith(".PCO")]
        
        # Create a list of full file paths
        file_paths = [os.path.join(folder_path, f) for f in cobol_files]

        # Use tqdm's process_map for progress tracking with multiprocessing
        process_map(process_file, file_paths, max_workers=max_workers, desc="Processing COBOL files", unit="file")
    else:
        print("No file path provided. Please provide a file path as a command-line argument.")