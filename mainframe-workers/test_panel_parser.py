"""
Description: 
A test script designed to parse COBOL Isuzu files with support for parallelism.

Usages:
1. Run the test script:
   `python test_cobol_parser_isuzu.py <folder_path>`

2. Check for COBOL program errors when failing to export JSON files:
   `cat <folder_path>/failed_files.log`

3. Remove all JSON files generated by the test script:
   `rm -rf <folder_path>/*.json && rm -rf <folder_path>/failed_files.log`

"""

import os
import re
import sys
import json
import codecs
import chardet

import warnings

# Ignore all warnings
warnings.filterwarnings("ignore")

from tqdm.contrib.concurrent import process_map
from antlr4 import CommonTokenStream, InputStream
from antlr4.error.ErrorListener import ErrorListener
from grammar.panel.PanelLexer import PanelLexer
from grammar.panel.PanelParser import PanelParser
from grammar.panel.PanelVisitorImp import PanelVisitorImp

# Initalize
max_workers = 4 # multiprocessing.cpu_count()

# Custom listener for checking the errors of Lexer (token errors)
class QuietErrorListener(ErrorListener):

    def __init__(self, verbose = False):
        super(QuietErrorListener, self).__init__()

    def syntaxError(self, recognizer, offendingSymbol, line, column, msg, e):
        pass

    def reportAmbiguity(self, recognizer, dfa, startIndex, stopIndex, exact, ambigAlts, configs):
        pass

    def reportAttemptingFullContext(self, recognizer, dfa, startIndex, stopIndex, conflictingAlts, configs):
        pass

    def reportContextSensitivity(self, recognizer, dfa, startIndex, stopIndex, prediction, configs):
        pass


# This function inherits from the repository parser
def beautify_lisp_string(in_string):
    indent_size = 3
    add_indent = ' '*indent_size
    out_string = in_string[0]  # no indent for 1st (
    indent = ''
    for i in range(1, len(in_string)):
        if in_string[i] == '(' and in_string[i+1] != ' ':
            indent += add_indent
            out_string += "\n" + indent + '('
        elif in_string[i] == ')':
            out_string += ')'
            if len(indent) > 0:
                indent = indent.replace(add_indent, '', 1)
        else:
            out_string += in_string[i]
    return out_string


# This function inherits from the repository parser

def extract_body_section(code: str):
    # Match ")BODY" line with any parameters (including WINDOW(6 7)), then capture content until next section
    pattern = r"(\)BODY[^\n]*\n)(.*?)(?=\n\)\w+)"
    match = re.search(pattern, code, re.DOTALL)

    if not match:
        return None, code
    
    body_header = match.group(1)
    body_text = match.group(2).strip()

    # Remove just the body text, keeping the header and next section
    cleaned_code = code.replace(match.group(0), body_header)
    
    return body_text, cleaned_code

def preprocess_panel(code):
    body_text, code = extract_body_section(code)
    # Remove all lines after the line ")END"
    end_index = code.find(")END")
    if end_index != -1:
        code = code[:end_index + 5]

    return code.replace("\u02dd=","=").replace("\u00fd=","=").replace("?=","=").replace("^=","="), body_text

def detect_encoding(file_path):
    try:
        with open(file_path, 'rb') as file:
            result = chardet.detect(file.read())
        if result['encoding']:
            return result['encoding']
        else:
            return "utf-8"
    except Exception as e:
        return "utf-8"

# with open(file_path, "rb") as f:
#     code = f.read().decode(decoding, errors="replace").replace("\uFFFD", " ")

 # Detect encoding

def read_file(file_path: str) -> str:
    encoding =  detect_encoding(file_path)
    print(encoding)
    # encoding = "shift_jis"
    # Read the file content
    with codecs.open(file_path, "rb") as f:
        raw_data = f.read()
        try:
            code = raw_data.decode(encoding, errors="replace").replace("\uFFFD", "^")
        except UnicodeDecodeError as e:
            print(f"Error decoding file: {e}")
            code = raw_data.decode("utf-8", errors="replace").replace("\uFFFD", "^")  # try UTF-8 as fallback
    return code
def parse_cbl(file_path:str)->None:

    # Get file name
    file_name = file_path.split("/")[-1]
    file_name_no_tail = file_name.split(".")[0]
    # Get contain folder
    folder_path = os.path.dirname(file_path)
    
    # Read file
    code = read_file(file_path)

    # Preprocess
    code,body_text = preprocess_panel(code)

    # Run lexer
    stream = InputStream(code)
    lexer = PanelLexer(stream)
    lexer.removeErrorListeners()  # Remove default error listeners
    lexer.addErrorListener(QuietErrorListener())  # Add custom error listener

    stream = CommonTokenStream(lexer)
    stream.fill()

    # Run parser
    parser = PanelParser(stream)
    parser.removeErrorListeners()  # Remove default error listeners
    parser.addErrorListener(QuietErrorListener())  # Add custom error listener
    parser.buildParseTrees = True

    # Build tree
    tree = parser.startRule()

    # Visit tree and Collect Information
    visitor = PanelVisitorImp(body_text)
    visitor.visit(tree)

    parsed_program = {
        "sections": [section.dict() for section in visitor.sections],
        # "statements": [statement.dict() for statement in visitor.statements],
    }

    # Save json file
    output_json_path = os.path.join(folder_path,f"{file_name_no_tail}.json")
    with open(output_json_path,"w", encoding="utf-8") as f:
        json.dump(parsed_program, f, ensure_ascii=False, indent=4)

    #print(f"Parsed program saved to {output_json_path}")


def log_failed_file(folder_path: str, file_path: str, error: str):
    """
    Logs details of files that failed during processing.

    Args:
        folder_path (str): The folder where the log file is saved.
        file_path (str): The file path of the failed file.
        error (str): The error message.
    """
    log_file_path = os.path.join(folder_path, "failed_files.log")
    with open(log_file_path, "a", encoding="utf-8") as log_file:
        log_file.write(f"Failed to process {file_path}: {error}\n")


def process_file(file_path):
    folder_path = os.path.dirname(file_path)
    try:
        # Parse the COBOL file
        parse_cbl(file_path)
    except Exception as e:
        # Log the error if parsing fails
        log_failed_file(folder_path, file_path, str(e))
        print(f"Error processing {file_path}: {e}")
    else:
        # If successful, print a success message
        print(f"Successfully processed {file_path}")



if __name__ == "__main__":

    folder_path = "tmp"

    if folder_path:
        print("Running...")
        # Get all files
        files = os.listdir(folder_path)
        # Filter files
        panel_files = [f for f in files if f.endswith('.txt')]
        
        # Create a list of full file paths
        file_paths = [os.path.join(folder_path, f) for f in panel_files]

        # Use tqdm's process_map for progress tracking with multiprocessing
        process_map(process_file, file_paths, max_workers=max_workers, desc="Processing PANEL files", unit="file")
    else:
        print("No file path provided. Please provide a file path as a command-line argument.")