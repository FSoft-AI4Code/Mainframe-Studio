"""
Description: 
A test script designed to parse COBOL Isuzu files with support for parallelism.

Usages:
1. Run the test script:
   `python test_cobol_parser_isuzu.py <folder_path>`

2. Check for COBOL program errors when failing to export JSON files:
   `cat <folder_path>/failed_files.log`

3. Remove all JSON files generated by the test script:
   `rm -rf <folder_path>/*.json && rm -rf <folder_path>/failed_files.log`

"""

import os
import re
import sys
import json
import codecs
import chardet

import warnings

# Ignore all warnings
warnings.filterwarnings("ignore")

from tqdm.contrib.concurrent import process_map
from antlr4 import CommonTokenStream, InputStream
from antlr4.error.ErrorListener import ErrorListener
from grammar.clist.ClistLexer import ClistLexer
from grammar.clist.ClistParser import ClistParser
from grammar.clist.ClistVisitorImp import ClistVisitorImp
import codecs
import chardet
def detect_encoding(file_path):
    try:
        with open(file_path, 'rb') as file:
            result = chardet.detect(file.read())
        if result['encoding']:
            return result['encoding']
        else:
            return "utf-8"
    except Exception as e:
        return "utf-8"

# Initalize
max_workers = 8 # multiprocessing.cpu_count()



class CustomLexerErrorListener(ErrorListener):

    def __init__(self, verbose = False):
        super(CustomLexerErrorListener, self).__init__()
        self.total_lexer_errors = 0
        self.verbose = verbose
        self.lexer_errors = []

    def syntaxError(self, recognizer, offendingSymbol, line, column, msg, e):
        """
        Called when the lexer encounters a syntax error (e.g., unrecognized characters).
        Logs the details of the offending symbol and location.
        """
        tag = "Lexer_Syntax_Error"
        location = f"Location {line}:{column}"
        if offendingSymbol:
            msg = f"{msg}. Offending Symbol: {offendingSymbol.text}."

        msg_ = f"[{tag}] " + msg
        if self.verbose:
            print(msg_)
        self.total_lexer_errors +=1
        # Collect errors
        self.lexer_errors.append((tag,location,msg))

    def reportAmbiguity(self, recognizer, dfa, startIndex, stopIndex, exact, ambigAlts, configs):
        """
        Placeholder for ambiguity reporting. Lexer rarely encounters ambiguities
        because tokenization rules are typically deterministic.
        """
        tag = "Lexer_Ambiguity"
        location = ""
        msg = f"Detected between indices {startIndex} and {stopIndex}."
        msg_ = f"[{tag}] " + msg
        if self.verbose:
            print(msg_)
        # Collect errors
        self.lexer_errors.append((tag,location,msg))

    def reportAttemptingFullContext(self, recognizer, dfa, startIndex, stopIndex, conflictingAlts, configs):
        """
        Placeholder for reporting when the lexer attempts full context resolution.
        Not typically applicable to lexers but included for completeness.
        """
        tag = "Lexer_Full_Context_Attempt"
        location = ""
        msg = f"StartIndex: {startIndex}, StopIndex: {stopIndex}"
        msg_ = f"[{tag}] " + msg
        if self.verbose:
            print(msg_)
        # Collect errors
        self.lexer_errors.append((tag,location,msg))

    def reportContextSensitivity(self, recognizer, dfa, startIndex, stopIndex, prediction, configs):
        """
        Placeholder for context sensitivity reporting. Context sensitivity is
        typically a parsing concern, but this function is included for symmetry.
        """
        tag = "Lexer_Context_Sensitivity"
        location = ""
        msg = f"StartIndex: {startIndex}, StopIndex: {stopIndex}, Prediction: {prediction}"
        msg_ = f"[{tag}] " + msg
        if self.verbose:
            print(msg_)
        # Collect errors
        self.lexer_errors.append((tag,location,msg))


# Custom listener for checking the errors of Parser
class CustomParserErrorListener(ErrorListener):

    def __init__(self,verbose=False):
        """
        Initialize the custom error listener with a token stream and lexer/parser.

        Args:
            token_stream: The token stream used by the parser.
            lexer_or_parser: The lexer or parser object to retrieve token names.
        """
        super(CustomParserErrorListener, self).__init__()
        self.total_parser_errors = 0
        self.verbose = verbose
        self.positions = []
        self.parser_errors = []
        
    def syntaxError(self, recognizer, offendingSymbol, line, column, msg, e):
        tag = "Parser_Syntax_Error"
        location = f"Location {line}:{column}"
        if offendingSymbol:
            msg = f"{msg}. Offending Token: {offendingSymbol.text}."

        msg_ = f"[{tag}] " + msg
        if self.verbose:
            print(msg_)
        # Collect errors
        self.parser_errors.append((tag,location,msg))
        self.total_parser_errors +=1
        self.positions.append((line,column))

    def reportAmbiguity(self, recognizer, dfa, startIndex, stopIndex, exact, ambigAlts, configs):
        tag = "Parser_Ambiguity"
        location = ""
        msg = f"Start Index {startIndex}, Stop Index {stopIndex}. Possible Alternatives: {ambigAlts}"
        msg_ = f"[{tag}] " + msg
        if self.verbose:
            print(msg_)
        # Collect errors
        self.parser_errors.append((tag,location,msg))

    def reportAttemptingFullContext(self, recognizer, dfa, startIndex, stopIndex, conflictingAlts, configs):
        tag = "Parser_Full_Context_Attempt"
        location = ""
        msg = f"Start Index {startIndex}, Stop Index {stopIndex}. Conflicting Alternatives: {conflictingAlts}"
        msg_ = f"[{tag}] " + msg
        if self.verbose:
            print(msg_)
        # Collect errors
        self.parser_errors.append((tag,location,msg))
        
    def reportContextSensitivity(self, recognizer, dfa, startIndex, stopIndex, prediction, configs):
        tag = "Parser_Context_Sensitivity"
        location = ""
        msg = f"Start Index {startIndex}, Stop Index {stopIndex}. Prediction: {prediction}"
        msg_ = f"[{tag}] " + msg
        if self.verbose:
            print(msg_)
        # Collect errors
        self.parser_errors.append((tag,location,msg))

# Custom listener for checking the errors of Lexer (token errors)
class QuietErrorListener(ErrorListener):

    def __init__(self, verbose = False):
        super(QuietErrorListener, self).__init__()

    def syntaxError(self, recognizer, offendingSymbol, line, column, msg, e):
        pass

    def reportAmbiguity(self, recognizer, dfa, startIndex, stopIndex, exact, ambigAlts, configs):
        pass

    def reportAttemptingFullContext(self, recognizer, dfa, startIndex, stopIndex, conflictingAlts, configs):
        pass

    def reportContextSensitivity(self, recognizer, dfa, startIndex, stopIndex, prediction, configs):
        pass


# This function inherits from the repository parser
def beautify_lisp_string(in_string):
    indent_size = 3
    add_indent = ' '*indent_size
    out_string = in_string[0]  # no indent for 1st (
    indent = ''
    for i in range(1, len(in_string)):
        if in_string[i] == '(' and in_string[i+1] != ' ':
            indent += add_indent
            out_string += "\n" + indent + '('
        elif in_string[i] == ')':
            out_string += ')'
            if len(indent) > 0:
                indent = indent.replace(add_indent, '', 1)
        else:
            out_string += in_string[i]
    return out_string


def modify_str(line):

    new_line = ""
    i = 0
    while i < len(line):
        if line[i:i+5] == "&STR(":
            # Found &STR(
            start = i + 5
            count = 1  # parentheses counter
            j = start
            while j < len(line) and count > 0:
                if line[j] == '(':
                    count += 1
                elif line[j] == ')':
                    count -= 1
                j += 1
            # Now line[start:j-1] is the inside content
            content = line[start:j-1]
            new_line += f'&STR("{content}")'
            i = j  # move pointer after the closing )
        else:
            new_line += line[i]
            i += 1

    return new_line

def preprocess_clist(code:str):
    L = 72
    code = code.replace("\u001E", "<RS>")
    code = "\n".join([line[:L] for line in code.splitlines() if line.strip()])
    code = code.replace("TOP ","")
    pattern1 = r"\+\s*\n"
    pattern2 = r"-\s*\n"
    code = re.sub(pattern1,"",code,flags=re.MULTILINE)
    code = re.sub(pattern2,"",code, flags=re.MULTILINE)
    lines = code.splitlines()
    for i,line in enumerate(lines):
        if "&STR(" in line:
            lines[i] = modify_str(line)
    code = "\n".join(lines)
    
    code = "\n".join([line.rstrip() for line in code.splitlines() if line.strip()])
    lines = code.splitlines()
    for i,line in enumerate(lines):
        if line.endswith("WRITE") or line.endswith("WRITENR"):
            lines[i] = lines[i] + "  "
    code = "\n".join(lines)
    code = code.replace("                 )'","").replace("POS3820C","").replace("POS3810C","").replace("POS3840C","")
    code = code.replace("<RS>","\u001E")
    if code.endswith("//"):
        code = code[:-2]
    return code


def read_file(file_path: str) -> str:
    """
    Reads the content of a file, detects its encoding, and returns the decoded string.
    
    Parameters:
        file_path (str): The path to the file to be read.
        
    Returns:
        str: The content of the file as a decoded string.
    """
    encoding = "utf-8"  # Default encoding

    # Detect file encoding
    try:
        with open(file_path, 'rb') as file:
            result = chardet.detect(file.read())
            encoding = result.get('encoding', 'utf-8')  # Use detected encoding or default to UTF-8
    except Exception as e:
        print(f"Error detecting file encoding: {e}")

    # Read and decode file content
    try:
        with codecs.open(file_path, "rb") as f:
            raw_data = f.read()
            try:
                return raw_data.decode(encoding, errors="replace").replace("\uFFFD", "")
            except UnicodeDecodeError as e:
                print(f"Error decoding with detected encoding ({encoding}): {e}")
                # Fallback to UTF-8 decoding
                return raw_data.decode("utf-8", errors="replace").replace("\uFFFD", "")
    except Exception as e:
        print(f"Error reading file: {e}")
        raise IOError(f"Failed to read or decode the file at {file_path}.") from e


def parse_clist(file_path:str)->None:

    # Get file name
    file_name = file_path.split("/")[-1]
    file_name_no_tail = file_name.split(".")[0]
    # Get contain folder
    folder_path = os.path.dirname(file_path)
    
# with open(file_path, "rb") as f:
#     code = f.read().decode(decoding, errors="replace").replace("\uFFFD", " ")

    # Detect encoding
    encoding =  detect_encoding(file_path)
    # print(encoding)
    encoding = "shift_jis"
    # Read the file content
    with codecs.open(file_path, "rb") as f:
        raw_data = f.read()
        try:
            code = raw_data.decode(encoding, errors="replace").replace("\uFFFD", "")
        except UnicodeDecodeError as e:
            print(f"Error decoding file: {e}")
            code = raw_data.decode("utf-8", errors="replace").replace("\uFFFD", "")  # try UTF-8 as fallback

    # Preprocess
    code = preprocess_clist(code)
    # print(code)
    # Run lexer
    stream = InputStream(code)
    lexer = ClistLexer(stream)
    # lexer.removeErrorListeners()  # Remove default error listeners
    # lexer.addErrorListener(QuietErrorListener())  # Add custom error listener
    lexer.removeErrorListeners()
    lexer_error_listener = CustomLexerErrorListener()
    lexer.addErrorListener(lexer_error_listener)
    stream = CommonTokenStream(lexer)
    stream.fill()

    # Run parser
    parser = ClistParser(stream)
    parser_error_listener = CustomParserErrorListener()
    parser.buildParseTrees = True
    parser.addErrorListener(parser_error_listener)

    # Build tree
    tree = parser.startRule()
    if lexer_error_listener.total_lexer_errors > 0 or parser_error_listener.total_parser_errors > 0:
        print(f"Errors found in file {file_name}:")
    # Visit tree and Collect Information
    visitor = ClistVisitorImp()
    visitor.visit(tree)
    # print(visitor.statements)

    parsed_program = {
        "statements": [statement.dict() for statement in visitor.statements],
        "labels": [label.dict() for label in visitor.labels]  
    }


    # statements = [DNPStatementFactory.from_dict(statement) for statement in visitor.statements]

    # parsed_program = {
    #     "copybook_list": [copybook.dict() for copybook in visitor.copybook_list],
    #     "called_program_list": [program.dict() for program in visitor.called_program_list],
    #     "file_control_list": [control.dict() for control in visitor.file_control_list],
    #     "file_description_list": [description.dict() for description in visitor.file_description_list],
    #     "variable_list": handle_variable_list(visitor.variable_list),
    #     "division_list": [division.dict() for division in visitor.division_list],
    #     "section_list": [section.dict() for section in visitor.section_list],
    #     "paragraph_list": [paragraph.dict() for paragraph in visitor.paragraph_list],
    #     "statements": [statement.dict() for statement in statements]

    # }

    # Save json file
    output_json_path = os.path.join(folder_path,f"{file_name_no_tail}.json")
    with open(output_json_path,"w", encoding="utf-8") as f:
        json.dump(parsed_program, f, ensure_ascii=False, indent=4)

    #print(f"Parsed program saved to {output_json_path}")


def log_failed_file(folder_path: str, file_path: str, error: str):
    """
    Logs details of files that failed during processing.

    Args:
        folder_path (str): The folder where the log file is saved.
        file_path (str): The file path of the failed file.
        error (str): The error message.
    """
    log_file_path = os.path.join(folder_path, "failed_files.log")
    with open(log_file_path, "a", encoding="utf-8") as log_file:
        log_file.write(f"Failed to process {file_path}: {error}\n")


def process_file(file_path):
    folder_path = os.path.dirname(file_path)
    # try:
    parse_clist(file_path)
    # except Exception as e:
    #     #print(f"Error parsing {file_path}: {e}")
    #     log_failed_file(folder_path, file_path, str(e))


if __name__ == "__main__":

    folder_path = "clist"  # Default folder path

    if folder_path:
        print("Running...")
        # Get all files
        files = os.listdir(folder_path)
        # Filter files
        cobol_files = [f for f in files if f.endswith("txt")]
        
        # Create a list of full file paths
        file_paths = [os.path.join(folder_path, f) for f in cobol_files]

        # Use tqdm's process_map for progress tracking with multiprocessing
        process_map(process_file, file_paths, max_workers=max_workers, desc="Processing COBOL files", unit="file")
    else:
        print("No file path provided. Please provide a file path as a command-line argument.")