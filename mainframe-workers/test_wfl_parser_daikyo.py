"""
Description: 
A test script designed to parse WFL files with support for parallelism.

Usages:
1. Run the test script:
   `python test_wfl_parser_daikyo.py <folder_path>`

2. Check for COBOL program errors when failing to export JSON files:
   `cat <folder_path>/failed_files.log`

3. Remove all JSON files generated by the test script:
   `rm -rf <folder_path>/*.json && rm -rf <folder_path>/failed_files.log`

"""

import os
import re
import sys
import json
import codecs
import chardet
import warnings

# Ignore all warnings
warnings.filterwarnings("ignore")

from tqdm.contrib.concurrent import process_map
from antlr4 import CommonTokenStream, InputStream
from antlr4.error.ErrorListener import ErrorListener

from grammar.wfl.WFLLexer import WFLLexer
from grammar.wfl.WFLParser import WFLParser
from grammar.wfl.WFLVisitorImp import WFLVisitorImp

# Initalize
max_workers = 16 # 8

# Custom listener for checking the errors of Lexer (token errors)
class QuietErrorListener(ErrorListener):

    def __init__(self, verbose = False):
        super(QuietErrorListener, self).__init__()

    def syntaxError(self, recognizer, offendingSymbol, line, column, msg, e):
        pass

    def reportAmbiguity(self, recognizer, dfa, startIndex, stopIndex, exact, ambigAlts, configs):
        pass

    def reportAttemptingFullContext(self, recognizer, dfa, startIndex, stopIndex, conflictingAlts, configs):
        pass

    def reportContextSensitivity(self, recognizer, dfa, startIndex, stopIndex, prediction, configs):
        pass

# These function inherits from the repository parser
def beautify_lisp_string(in_string):
    indent_size = 3
    add_indent = ' '*indent_size
    out_string = in_string[0]  # no indent for 1st (
    indent = ''
    for i in range(1, len(in_string)):
        if in_string[i] == '(' and in_string[i+1] != ' ':
            indent += add_indent
            out_string += "\n" + indent + '('
        elif in_string[i] == ')':
            out_string += ')'
            if len(indent) > 0:
                indent = indent.replace(add_indent, '', 1)
        else:
            out_string += in_string[i]
    return out_string


def preprocess_wfl(code: str) -> str:
    # Process each line to remove 8 digits at the end of lines
    filtered_lines = []
    for line in code.splitlines():
        # Skip lines that start with %***%
        if line.strip().startswith('%***%'):
            continue
        
        # Remove 8 digits at the end of the line if they exist
        # This pattern matches exactly 8 digits at the end of a line, 
        # but not if they're inside quotes or part of a larger string
        line = re.sub(r'(\s+)\d{8}\s*$', r'\1', line)
        
        # Only add non-empty lines
        if line.strip():
            filtered_lines.append(line.rstrip())
    
    # Join the remaining lines
    code = "\n".join(filtered_lines)

    code = code.replace("/FROM/", "/FROMCONSTANTVALUE/").replace('""""','""').replace('"""','"').replace("?????", "").replace('""09""', "09").replace('""00""',"00")
    
    return code

# Helper functions
def read_file(file_path: str) -> str:
    """
    Reads the content of a file, detects its encoding, and returns the decoded string.
    
    Parameters:
        file_path (str): The path to the file to be read.
        
    Returns:
        str: The content of the file as a decoded string.
    """
    encoding = "utf-8"  # Default encoding

    # Detect file encoding
    try:
        with open(file_path, 'rb') as file:
            result = chardet.detect(file.read())
            detected_encoding = result.get('encoding')
            if detected_encoding is not None:
                encoding = detected_encoding
    except Exception as e:
        print(f"Error detecting file encoding: {e}")

    # Read and decode file content
    try:
        with codecs.open(file_path, "rb") as f:
            raw_data = f.read()
            try:
                return raw_data.decode(encoding, errors="replace").replace("\uFFFD", "")
            except UnicodeDecodeError as e:
                print(f"Error decoding with detected encoding ({encoding}): {e}")
                # Fallback to UTF-8 decoding
                return raw_data.decode("utf-8", errors="replace").replace("\uFFFD", "")
    except Exception as e:
        print(f"Error reading file: {e}")
        raise IOError(f"Failed to read or decode the file at {file_path}.") from e
    
def log_failed_file(folder_path: str, file_path: str, error: str):
    """
    Logs details of files that failed during processing.

    Args:
        folder_path (str): The folder where the log file is saved.
        file_path (str): The file path of the failed file.
        error (str): The error message.
    """
    log_file_path = os.path.join(folder_path, "failed_files.log")
    with open(log_file_path, "a", encoding="utf-8") as log_file:
        log_file.write(f"Failed to process {file_path}: {error}\n")
    
def parse_wfl(file_path:str)->None:

    # Init
    encoding = "shift_jis" #"utf-8"

    # Get file name
    file_name = file_path.split("/")[-1]
    file_name_no_tail = file_name.split(".")[0]

    # Get contain folder
    folder_path = os.path.dirname(file_path)
    
    # Detect encoding
    # try:
    #     with open(file_path, 'rb') as file:
    #         result = chardet.detect(file.read())
    #         detected_encoding = result.get('encoding')
    #         if detected_encoding is not None:
    #             encoding = detected_encoding
    # except Exception as e:
    #     print(f"Error detecting file encoding: {e}")

    # Read file
    #code = read_file(file_path)
    with open(file_path, "r", encoding="shift_jis") as f:
        code = f.read()

    # Preprocess
    code = preprocess_wfl(code)

    # Run lexer
    stream = InputStream(code)
    lexer = WFLLexer(stream)
    lexer.removeErrorListeners()  # Remove default error listeners
    lexer.addErrorListener(QuietErrorListener())  # Add custom error listener

    token_stream = CommonTokenStream(lexer)
    token_stream.fill()

    # Get comments
    comments = []
    # Get symbolic names list from the lexer
    symbolic_names = lexer.symbolicNames

    # Print token details
    for token in token_stream.tokens:
        token_name = symbolic_names[token.type] if token.type < len(symbolic_names) else "UNKNOWN"
        if token_name == "COMMENT":
            cmt = {
                "text": token.text,
                "index": token.tokenIndex,
                "line": token.line,
                "column": token.column,
                "start": token.start,
                "stop": token.stop
            }
            comments.append(cmt)

    # Run parser
    parser = WFLParser(token_stream)
    parser.removeErrorListeners()  # Remove default error listeners
    parser.addErrorListener(QuietErrorListener())  # Add custom error listener
    parser.buildParseTrees = True

    # Build tree
    tree = parser.startRule()

    # Visit tree and Collect Information
    visitor = WFLVisitorImp()
    visitor.visit(tree)

    # Get Parsed Information
    job_name = visitor.job_name
    subroutines = [subroutine.dict() for subroutine in visitor.subroutines]
    statements = [ statement.dict() for statement in visitor.statements]
    attributes =[attr.dict() for attr in visitor.attributes]
    parameters = [param.dict() for param in visitor.parameters]
    declarations = [dec.dict() for dec in visitor.declarations]

    parsed_program = {"encoding": encoding,
                      "job_name": job_name,
                      "parameters": parameters,
                      "attributes": attributes,
                      "declarations": declarations,
                      "subroutines": subroutines,
                      "statements": statements,
                      "comments": comments
    }

    # Save json file
    output_json_path = os.path.join(folder_path,f"{file_name_no_tail}.json")
    with open(output_json_path,"w", encoding='shift_jis') as f:
        json.dump(parsed_program, f, ensure_ascii=False, indent=4)
        
    #print(f"Parsed program saved to {output_json_path}")

def process_file(file_path):
    folder_path = os.path.dirname(file_path)
    try:
        parse_wfl(file_path)
    except Exception as e:
        print(f"Error parsing {file_path}: {e}")
        log_failed_file(folder_path, file_path, str(e))

if __name__ == "__main__":

    folder_path = sys.argv[1] if len(sys.argv) > 1 else None

    if folder_path:
        print("Running...")
        # Get all files
        files = os.listdir(folder_path)
        # Filter files
        cobol_files = [f for f in files if f.endswith(".txt")]
        
        # Create a list of full file paths
        file_paths = [os.path.join(folder_path, f) for f in cobol_files]

        # Use tqdm's process_map for progress tracking with multiprocessing
        process_map(process_file, file_paths, max_workers=max_workers, desc="Processing WFL files", unit="file")
    else:
        print("No file path provided. Please provide a file path as a command-line argument.")
