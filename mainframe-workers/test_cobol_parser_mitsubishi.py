"""
Description: 
A test script designed to parse COBOL Isuzu files with support for parallelism.

Usages:
1. Run the test script:
   `python test_cobol_parser_isuzu.py <folder_path>`

2. Check for COBOL program errors when failing to export JSON files:
   `cat <folder_path>/failed_files.log`

3. Remove all JSON files generated by the test script:
   `rm -rf <folder_path>/*.json && rm -rf <folder_path>/failed_files.log`

"""

import os
import re
import sys
import json
import codecs
import chardet

import warnings

# Ignore all warnings
warnings.filterwarnings("ignore")

from tqdm.contrib.concurrent import process_map
from antlr4 import CommonTokenStream, InputStream
from antlr4.error.ErrorListener import ErrorListener
from grammar.ibm_cobol.Cobol85Lexer import Cobol85Lexer
from grammar.ibm_cobol.Cobol85Parser import Cobol85Parser
from grammar.ibm_cobol.MyCobol85Visitor import MyCobol85Visitor
from grammar.ibm_cobol.ibm_cobol_schemas import IBMStatementFactory

# Initalize
max_workers = 4 # multiprocessing.cpu_count()

# Custom listener for checking the errors of Lexer (token errors)
class QuietErrorListener(ErrorListener):

    def __init__(self, verbose = False):
        super(QuietErrorListener, self).__init__()

    def syntaxError(self, recognizer, offendingSymbol, line, column, msg, e):
        pass

    def reportAmbiguity(self, recognizer, dfa, startIndex, stopIndex, exact, ambigAlts, configs):
        pass

    def reportAttemptingFullContext(self, recognizer, dfa, startIndex, stopIndex, conflictingAlts, configs):
        pass

    def reportContextSensitivity(self, recognizer, dfa, startIndex, stopIndex, prediction, configs):
        pass


# This function inherits from the repository parser
def beautify_lisp_string(in_string):
    indent_size = 3
    add_indent = ' '*indent_size
    out_string = in_string[0]  # no indent for 1st (
    indent = ''
    for i in range(1, len(in_string)):
        if in_string[i] == '(' and in_string[i+1] != ' ':
            indent += add_indent
            out_string += "\n" + indent + '('
        elif in_string[i] == ')':
            out_string += ')'
            if len(indent) > 0:
                indent = indent.replace(add_indent, '', 1)
        else:
            out_string += in_string[i]
    return out_string


# This function inherits from the repository parser
def preprocess_cbl_misubishi(code: str) -> str:
    pattern = re.compile(r'\b0\d{7}\b')        # Replace matched patterns with an empty string    
    code = pattern.sub('', code)
    L = 72
    code = "\n".join([line[:L] for line in code.splitlines() if line.strip()])
    # Remove labels
    lines = code.splitlines()
    for i, line in enumerate(lines):
        if len(line) < 7:
            # Remove lines with less than 7 characters
            lines[i] = ""
        if len(line) >= 6 and not line.startswith("      "):
            lines[i] = "      " + line[6:]
    code = "\n".join(lines)

    # Remove row index Ex 00010000
    # Regex to detect the line number at the beginning of each line (e.g., 000100)
    code = re.sub(r"^.{6}", "      ", code, flags=re.MULTILINE)
    code = re.sub(r"^.{6}\/", "      *", code, flags=re.MULTILINE)
    code = re.sub(r"^\s{7}\*", "      **", code, flags=re.MULTILINE)
    code = re.sub(r'\*>.*$', '', code, flags=re.MULTILINE)

    # Remove 8-digit pattern at the tail of each row
    code = re.sub(r"\d{8}$", "", code, flags=re.MULTILINE)  # Matches an 8-digit number at the end of a line

    # Remove lines start with something
    code = '\n'.join([line for line in code.splitlines() if not line.strip().startswith("AUTHOR")])  # Remove lines starting with "AUTHOR"
    code = '\n'.join([line for line in code.splitlines() if not line.strip().startswith("DATE-WRITTEN")])  # Remove lines starting with "DATE-WRITTEN"
    code = '\n'.join([line for line in code.splitlines() if not line.strip().startswith("DATA-WRITTEN")])  # Remove lines starting with "DATA-WRITTEN"
    code = '\n'.join([line for line in code.splitlines() if not line.strip().startswith("DATA-WRITTEN")])  # Remove lines starting with "DATA-WRITTEN"
    code = '\n'.join([line for line in code.splitlines() if not line.strip().startswith("DATA-COMPILED")])  # Remove lines starting with "DATA-COMPILED"

    # Remove line only have SKIP3, EJECT
    code = '\n'.join([line for line in code.splitlines() if not line.strip() == "SKIP3"])
    code = '\n'.join([line.rstrip() for line in code.splitlines() if not line.strip() == "EJECT"])
    
    code = code.replace("      D    ", "       "). replace("REMARKS *  *********************************************", "REMARKS")
    return code


def read_file(file_path: str) -> str:
    """
    Reads the content of a file, detects its encoding, and returns the decoded string.
    
    Parameters:
        file_path (str): The path to the file to be read.
        
    Returns:
        str: The content of the file as a decoded string.
    """
    encoding = "utf-8"  # Default encoding

    # Detect file encoding
    try:
        with open(file_path, 'rb') as file:
            result = chardet.detect(file.read())
            encoding = result.get('encoding', 'utf-8')  # Use detected encoding or default to UTF-8
    except Exception as e:
        print(f"Error detecting file encoding: {e}")

    # Read and decode file content
    try:
        with codecs.open(file_path, "rb") as f:
            raw_data = f.read()
            try:
                return raw_data.decode(encoding, errors="replace").replace("\uFFFD", "")
            except UnicodeDecodeError as e:
                print(f"Error decoding with detected encoding ({encoding}): {e}")
                # Fallback to UTF-8 decoding
                return raw_data.decode("utf-8", errors="replace").replace("\uFFFD", "")
    except Exception as e:
        print(f"Error reading file: {e}")
        raise IOError(f"Failed to read or decode the file at {file_path}.") from e


def handle_variable_list(variable_list):
    output = {}
    for variable in variable_list:
        n, L  = variable # field and list of objects
        output[n] = [l.dict() for l in L]
    return output


def parse_cbl(file_path:str)->None:

    # Get file name
    file_name = file_path.split("/")[-1]
    file_name_no_tail = file_name.split(".")[0]
    # Get contain folder
    folder_path = os.path.dirname(file_path)
    
    # Read file
    code = read_file(file_path)

    # Preprocess
    code = preprocess_cbl_misubishi(code)

    # Run lexer
    stream = InputStream(code)
    lexer = Cobol85Lexer(stream)
    lexer.removeErrorListeners()  # Remove default error listeners
    lexer.addErrorListener(QuietErrorListener())  # Add custom error listener

    stream = CommonTokenStream(lexer)
    stream.fill()

    # Run parser
    parser = Cobol85Parser(stream)
    parser.removeErrorListeners()  # Remove default error listeners
    parser.addErrorListener(QuietErrorListener())  # Add custom error listener
    parser.buildParseTrees = True

    # Build tree
    tree = parser.startRule()

    # Visit tree and Collect Information
    visitor = MyCobol85Visitor(parser=parser)
    visitor.visit(tree)

    statements = [IBMStatementFactory.from_dict(statement) for statement in visitor.statements]

    parsed_program = {
        "copybook_list": [copybook.dict() for copybook in visitor.copybook_list],
        "called_program_list": [program.dict() for program in visitor.called_program_list],
        "file_control_list": [control.dict() for control in visitor.file_control_list],
        "file_description_list": [description.dict() for description in visitor.file_description_list],
        "variable_list": handle_variable_list(visitor.variable_list),
        "division_list": [division.dict() for division in visitor.division_list],
        "section_list": [section.dict() for section in visitor.section_list],
        "paragraph_list": [paragraph.dict() for paragraph in visitor.paragraph_list],
        "statements": [statement.dict() for statement in statements]

    }

    # Save json file
    output_json_path = os.path.join(folder_path,f"{file_name_no_tail}.json")
    with open(output_json_path,"w", encoding="utf-8") as f:
        json.dump(parsed_program, f, ensure_ascii=False, indent=4)

    #print(f"Parsed program saved to {output_json_path}")


def log_failed_file(folder_path: str, file_path: str, error: str):
    """
    Logs details of files that failed during processing.

    Args:
        folder_path (str): The folder where the log file is saved.
        file_path (str): The file path of the failed file.
        error (str): The error message.
    """
    log_file_path = os.path.join(folder_path, "failed_files.log")
    with open(log_file_path, "a", encoding="utf-8") as log_file:
        log_file.write(f"Failed to process {file_path}: {error}\n")


def process_file(file_path):
    folder_path = os.path.dirname(file_path)
    try:
        parse_cbl(file_path)
    except Exception as e:
        #print(f"Error parsing {file_path}: {e}")
        log_failed_file(folder_path, file_path, str(e))


if __name__ == "__main__":

    folder_path = "tmp"

    if folder_path:
        print("Running...")
        # Get all files
        files = os.listdir(folder_path)
        # Filter files
        cobol_files = [f for f in files]
        
        # Create a list of full file paths
        file_paths = [os.path.join(folder_path, f) for f in cobol_files]

        # Use tqdm's process_map for progress tracking with multiprocessing
        process_map(process_file, file_paths, max_workers=max_workers, desc="Processing COBOL files", unit="file")
    else:
        print("No file path provided. Please provide a file path as a command-line argument.")